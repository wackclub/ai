<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hack Club | AI</title>
  </head>
  <body>
    <header>
      <h1>ai.hackclub.com</h1>
      <p>
        An experimental service providing unlimited
        <code>/chat/completions</code> for free, for teens in
        <a href="https://hackclub.com/" target="_blank">Hack Club</a>. No API
        key needed.
      </p>
      <p>
        <b>{{ total_tokens }}</b> tokens processed since January 2025. Current
        model:
        <b
          ><code>{{ model }}</code></b
        >.
      </p>
      <p>
        Open source at
        <a href="https://github.com/hackclub/ai">github.com/hackclub/ai</a>!
      </p>
    </header>

    <section>
      <h2>Usage</h2>
      <h3>Chat Completions</h3>
      <pre><code>curl -X POST https://ai.hackclub.com/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "messages": [{"role": "user", "content": "Tell me a joke!"}]
    }'</code></pre>

      <h3>Get Current Model</h3>
      <p>To get current model:</p>
      <pre><code>curl https://ai.hackclub.com/model</code></pre>
      <p>Example response: <code>{{ model }}</code></p>
    </section>

    <section>
      <h2>API Parameters</h2>
      <p>The API accepts all standard OpenAI-style parameters for chat completions:</p>
      <ul>
        <li><code>messages</code> - Array of message objects with role and content</li>
        <li><code>model</code> - (Ignored) Model is set server-side</li>
        <li><code>temperature</code> - Controls randomness (0.0 to 2.0)</li>
        <li><code>max_completion_tokens</code> - Maximum number of tokens to generate</li>
        <li><code>top_p</code> - Controls diversity via nucleus sampling</li>
        <li><code>frequency_penalty</code> - Reduces repetition of token sequences</li>
        <li><code>presence_penalty</code> - Reduces repetition of tokens</li>
        <li><code>stream</code> - Set to true for streaming responses</li>
        <li><code>stop</code> - Array of tokens at which to stop generation</li>
        <li><code>response_format</code> - Format of the response (e.g., JSON mode)</li>
        <li><code>seed</code> - Integer for random seed to enable deterministic sampling</li>
        <li><code>tools</code> - List of tools the model may call</li>
        <li><code>tool_choice</code> - Controls which (if any) tool is called</li>
        <li><code>include_reasoning</code> - Set to true to include reasoning in responses (reasoning models only)</li>
        <li><code>reasoning_format</code> - Format for reasoning output (reasoning models only)</li>
      </ul>
      <p><strong>Note:</strong> The <code>model</code> parameter is ignored as the model is configured server-side.</p>
      <p><strong>Note:</strong> Reasoning parameters (<code>include_reasoning</code> and <code>reasoning_format</code>) only work when the currently set model is a reasoning model.</p>
    </section>

    <section>
	<h2>Terms</h2>
	<p>
	  You must be a teenager in the <a href="https://hackclub.com/slack">Hack Club Slack</a>.
	  All requests and responses are logged to prevent abuse.
	  Projects only — no personal use. This means you can't use it in Cursor or anything similar for the moment!
	  Abuse means this will get shut down — we're a nonprofit funded by donations.
	</p>
    </section>
  </body>
</html>